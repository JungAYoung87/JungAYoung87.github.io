# 머신러닝 5차 과제

## 과제 1

조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

### 기본 설정

 필수 모듈 불러오기

 그래프 출력 관련 기본 설정 지정

```python
#파이썬 ≥3.5 필수
import sys
assert sys.version_info >= (3, 5) 

#사이킷런 ≥0.20 필수
import sklearn
assert sklearn.__version__ >= "0.20"

#공통 모듈 임포트
import numpy as np
import os

#노트북 실행 결과를 동일하게 유지하기 위해
np.random.seed(42)

#깔끔한 그래프 출력을 위해
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

#그림을 저장할 위치
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "svm"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("그림 저장:", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
   ```
   
### 데이터 준비
   
붓꽃 데이터셋의 꽃잎 길이와 꽃잎 너비 특성만 이용한다
   
```python
from sklearn import datasets
iris = datasets.load_iris()

iris.target

X = iris["data"][:, (2, 3)]                  # 꽃잎 길이와 너비
y = (iris["target"] == 2).astype(np.int)
```

#### 편향 추가

수식을 행렬 연산으로 보다 간단하게 처리하기 위해 모든 샘플에 편향을 추가한다.

```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

#### 랜덤 시드 지정

결과를 일정하게 유지하기 위해 랜덤 시드를 지정한다

```python
np.random.seed(2042)
```

### 데이터셋 분할

데이터셋을 훈련, 검증, 테스트 용도로 6대2대2의 비율로 무작위 분할한다.

훈련 세트:60%

검증 세트:20%

테스트 세트:20%

```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

#### 인덱스 무작위로 섞기

```python
rnd_indices = np.random.permutation(total_size)

X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]

y_train[:5]
```

### 로지스틱 모델 구현

#### 시그모이드 함수 만들기


```python
def sigmoid(X):
    return 1 / (1+np.exp(-X))

def loss_func(X, t):
    
    delta = 1e-7    # log 무한대 발산 방지
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    # cross-entropy 
    return  -np.sum( y*np.log(t + delta) + (1-y)*np.log((1 - t)+delta ) )
```

```python
def numerical_derivative(f, X):
    delta_X = 1e-4 # 0.0001
    grad = np.zeros_like(X)
    
    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])
    
    while not it.finished:
        idx = it.multi_index        
        tmp_val = X[idx]
        X[idx] = float(tmp_val) + delta_X
        fX1 = f(X) # f(X+delta_X)
        
        X[idx] = tmp_val - delta_X 
        fX2 = f(X) # f(X-delta_X)
        grad[idx] = (fX1 - fX2) / (2*delta_X)
        
        X[idx] = tmp_val 
        it.iternext()   
        
    return grad
```
    
```python
    def error_val(X, y):
    delta = 1e-7    # log 무한대 발산 방지
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    # cross-entropy 
    return  -np.sum( y*np.log(t + delta) + (1-y)*np.log((1 - t)+delta ) ) 

# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수
# 입력변수 X : numpy type
def predict(X):
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    if t >= 0.5:
        result = 1  # True
    else:
        result = 0  # False
    
    return t, result
```

### 경사하강법 활용 훈련

경사하강법을 구현하기 위해 비용함수와 비용함수의 그레이디언트를 파이썬으로 구현해야 한다.

배치 경사하강법 훈련은 아래 코드를 통해 이루어진다.

- `eta = 0.01`: 학습률
- `n_iterations = 5001` : 에포크 수
- `m = len(X_train)`: 훈련 세트 크기, 즉 훈련 샘플 수
- `epsilon = 1e-7`: log 값이 항상 계산되도록 더해지는 작은 실수
- `logits`: 모든 샘플에 대한 클래스별 점수
- `Y_proba`: 모든 샘플에 대해 계산된 클래스 별 소속 확률

```python
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개
```

```python
#  배치 경사하강법 구현
eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot     # 그레이디언트 계산.
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```

```python
Theta
```

### 검증

검증세트로 모델 성능을 판단한다

Y_proba값이 0.5 이상이면 버지니아로, 아니라면 버지니아가 아니라고 입력해준다.

```python
X = X_valid.dot(Theta)              
Y_proba = sigmoid(X)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

### 규제가 추가된 경사하강법 활용 훈련

코드는 기본적으로 동일하지만 손실에 페널티가 추가되었고 그래디언트에도 항이 추가된다

-학습률 eta 증가

-alpha = 0.1 : 규제 강도

```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```
    
### 검증 세트를 이용한 성능 확인

```python
X = X_valid.dot(Theta)              
Y_proba = sigmoid(X)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

### 조기 종료 추가

조기 종료는 검증세트에 대한 손실값이 이전 단계보다 커지면 바로 종료되는 기능이다.

매 에포크마다 검증 세트에 대한 손실을 계산하여 오차가 줄어들다가 증가하기 시작할 때 멈추도록 한다.

```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    X = X_valid.dot(Theta)
    Y_proba = sigmoid(X)
    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

### 테스트 세트 평가

```python
X = X_test.dot(Theta)
Y_proba = sigmoid(X)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)


accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```

### 사이킷런 로지스틱 모델과 성능 비교

```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X_train, y_train)
```

```python
log_reg.score(X_test,y_test)
```

```python
from sklearn.metrics import accuracy_score
y_pred = log_reg.predict(X_test)
accuracy_score(y_test, y_pred)
```
    
## 과제2

과제1에서 구현된 로지스틱 회귀 알고리즘에 일대다 방식을 적용하여 붓꽃에 대한 다중 클래스 분류 알고리즘을 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

로지스틱 모델 2개 사용

- setosa인지 아닌지를 판단하는 모델

- verginica인지 아닌지를 판단하는 모델

versicolor일 확률은 '1 - setosa일 확률 - verginica일 확률'로 계산

### 데이터셋 준비

```python
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이
y = iris["target"]
y0 = (iris["target"] == 0).astype(np.int) #setosa 판단 모델을 위한 데이터셋
y1 = (iris["target"] == 2).astype(np.int) #virginica 판단 모델을 위한 데이터셋
```

```python
X_with_bias = np.c_[np.ones([len(X), 1]), X] #편향추가
```

```python
np.random.seed(2042) #일정한 결과를 위해 랜덤시드 지정
```

모델 훈련은 각 클래스에 대해 각각 이루어지기 때문에 데이터셋도 개별적으로 준비한다.

```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

```python
rnd_indices = np.random.permutation(total_size) #데이터 섞기
```

### setosa 판별 로지스틱 회귀 모델

과제1에서 구현한 로지스틱 회귀 모델에 setosa에 대한 라벨을 적용하여 setosa 판별 로지스틱 회귀 모델을 구현한다.

```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss0 = np.infty   # 최소 손실값 기억 변수

Theta0 = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits0 = X_train.dot(Theta0)
    Y_proba0 = logistic(logits0)
    error = Y_proba0 - y_train0
    gradients0 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta0[1:]]
    Theta0 = Theta0 - eta * gradients0

    # 검증 세트에 대한 손실 계산
    logits0 = X_valid.dot(Theta0)
    Y_proba0 = logistic(logits0)
    xentropy_loss0 = -np.mean(np.sum((y_valid0*np.log(Y_proba0 + epsilon) + (1-y_valid0)*np.log(1 - Y_proba0 + epsilon))))
    l2_loss0 = 1/2 * np.sum(np.square(Theta0[1:]))
    loss0 = xentropy_loss0 + alpha * l2_loss0
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss0)
        
    # 에포크마다 최소 손실값 업데이트
    if loss0 < best_loss0:
        best_loss0 = loss0
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss0)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss0, "조기 종료!")
        break
```

### virginica 판별 로지스틱 회귀 모델

과제1에서 구현한 로지스틱 회귀 모델에 verginica에 대한 라벨을 적용하여 virginica 판별 로지스틱 회귀 모델을 구현한다.

```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss1 = np.infty   # 최소 손실값 기억 변수

Theta1 = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits1 = X_train.dot(Theta1)
    Y_proba1 = logistic(logits1)
    error = Y_proba1 - y_train1
    gradients1 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta1[1:]]
    Theta1 = Theta1 - eta * gradients1

    # 검증 세트에 대한 손실 계산
    logits1 = X_valid.dot(Theta1)
    Y_proba1 = logistic(logits1)
    xentropy_loss1 = -np.mean(np.sum((y_valid1*np.log(Y_proba1 + epsilon) + (1-y_valid1)*np.log(1 - Y_proba1 + epsilon))))
    l2_loss1 = 1/2 * np.sum(np.square(Theta1[1:]))
    loss1 = xentropy_loss1 + alpha * l2_loss1
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss1)
        
    # 에포크마다 최소 손실값 업데이트
    if loss1 < best_loss1:
        best_loss1 = loss1
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss1)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss1, "조기 종료!")
        break
```

### 테스트셋 적용

setosa일 확률, virginica일 확률, versicolor일 확률 중 가장 높은 것을 채택하여 분류한다.

```python
logits = X_test.dot(Theta0) #setosa에 대한 확률값 추정  
setosa_proba = logistic(logits)

logits = X_test.dot(Theta1) #virginica에 대한 확률값 추정 
virginica_proba = logistic(logits)

y_predict = np.array([])
for i in range(len(Y_proba0)):
  prob_list = [[setosa_proba[i], 0], [1-setosa_proba[i]-virginica_proba[i], 1], [virginica_proba[i], 2]]
  prob_list.sort(reverse=True) #가장 높은 확률이 가장 앞으로 오게끔 정렬해준다.
  y_predict = np.append(y_predict, prob_list[0][1]) #가장 확률이 높았던 것을 예측값으로 결정한다.
  
  accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```

### 사이킷런의 로지스틱 모델과의 성능 비교

```python
from sklearn.linear_model import LogisticRegression
multi_log_reg = LogisticRegression(solver='newton-cg', random_state=42).fit(X_train,y_train)

multi_log_reg.score(X_test,y_test)
```
