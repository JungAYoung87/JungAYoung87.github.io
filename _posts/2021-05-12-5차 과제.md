# 머신러닝 5차 과제

## 과제 1

조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

### 기본 설정

 필수 모듈 불러오기

 그래프 출력 관련 기본 설정 지정

```python
#파이썬 ≥3.5 필수
import sys
assert sys.version_info >= (3, 5) 

#사이킷런 ≥0.20 필수
import sklearn
assert sklearn.__version__ >= "0.20"

#공통 모듈 임포트
import numpy as np
import os

#노트북 실행 결과를 동일하게 유지하기 위해
np.random.seed(42)

#깔끔한 그래프 출력을 위해
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

#그림을 저장할 위치
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "svm"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("그림 저장:", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
   ```
   
### 데이터 준비
   
붓꽃 데이터셋의 꽃잎 길이와 꽃잎 너비 특성만 이용한다
   
```python
from sklearn import datasets
iris = datasets.load_iris()

iris.target

X = iris["data"][:, (2, 3)]                  # 꽃잎 길이와 너비
y = (iris["target"] == 2).astype(np.int)
```

#### 편향 추가

```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

#### 랜덤 시드 지정

```python
np.random.seed(2042)
```

### 데이터셋 분할

훈련 세트:60%

검증 세트:20%

테스트 세트:20%

```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

#### 인덱스 무작위로 섞기

```python
rnd_indices = np.random.permutation(total_size)

X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]

y_train[:5]
```

### 로지스틱 모델 구현

#### 시그모이드 함수 만들기

```python
def sigmoid(X):
    return 1 / (1+np.exp(-X))

def loss_func(X, t):
    
    delta = 1e-7    # log 무한대 발산 방지
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    # cross-entropy 
    return  -np.sum( y*np.log(t + delta) + (1-y)*np.log((1 - t)+delta ) )
```

```python
def numerical_derivative(f, X):
    delta_X = 1e-4 # 0.0001
    grad = np.zeros_like(X)
    
    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])
    
    while not it.finished:
        idx = it.multi_index        
        tmp_val = X[idx]
        X[idx] = float(tmp_val) + delta_X
        fX1 = f(X) # f(X+delta_X)
        
        X[idx] = tmp_val - delta_X 
        fX2 = f(X) # f(X-delta_X)
        grad[idx] = (fX1 - fX2) / (2*delta_X)
        
        X[idx] = tmp_val 
        it.iternext()   
        
    return grad
```
    
```python
    def error_val(X, y):
    delta = 1e-7    # log 무한대 발산 방지
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    # cross-entropy 
    return  -np.sum( y*np.log(t + delta) + (1-y)*np.log((1 - t)+delta ) ) 

# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수
# 입력변수 X : numpy type
def predict(X):
    
    z = np.dot(X,W) + b
    t = sigmoid(z)
    
    if t >= 0.5:
        result = 1  # True
    else:
        result = 0  # False
    
    return t, result
```

### 경사하강법 활용 훈련

```python
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개
```

```python
#  배치 경사하강법 구현
eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot     # 그레이디언트 계산.
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```

```python
Theta
```

### 검증

```python
logits = X_valid.dot(Theta)              
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

### 규제가 추가된 경사하강법 활용 훈련

```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```
    
### 검증 세트를 이용한 성능 확인

```python
logits = X_valid.dot(Theta)              
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

### 조기 종료 추가

```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    X = X_train.dot(Theta)
    Y_proba = sigmoid(X)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    X = X_valid.dot(Theta)
    Y_proba = sigmoid(X)
    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

### 테스트 세트 평가

```python
logits = X_test.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)


accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```

### 사이킷런 로지스틱 모델과 성능 비교

```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X_train, y_train)
```

```python
log_reg.score(X_test,y_test)
```

```python
from sklearn.metrics import accuracy_score
y_pred = log_reg.predict(X_test)
accuracy_score(y_test, y_pred)
```
    
## 과제2

과제1에서 구현된 로지스틱 회귀 알고리즘에 일대다 방식을 적용하여 붓꽃에 대한 다중 클래스 분류 알고리즘을 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

로지스틱 모델 2개 사용

- setosa인지 아닌지를 판단하는 모델

- verginica인지 아닌지를 판단하는 모델

versicolor일 확률은 '1 - setosa일 확률 - verginica일 확률'로 계산
